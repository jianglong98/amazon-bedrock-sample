{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Latency Benchmarking Framework\n",
    "\n",
    "This notebook provides a comprehensive framework for benchmarking latency in Large Language Models (LLMs) through Amazon Bedrock. It enables systematic testing of:\n",
    "- Standard vs. Optimized model variants\n",
    "- Concurrent API calls\n",
    "- Different workload patterns\n",
    "- Various model configurations\n",
    "\n",
    "The framework measures two critical performance metrics:\n",
    "\n",
    "1. **Time to First Token (TTFT)**: How quickly the model starts responding\n",
    "   - Lower is better\n",
    "   - Affected by prompt length and network conditions\n",
    "   \n",
    "2. **Output Tokens Per Second (OTPS)**: Generation throughput after the model starts responding\n",
    "   - Higher is better\n",
    "   - Affected by prompt complexity, task complexity, and model intelligence\n",
    "\n",
    "> **⚠️ Important**: Latency benchmarks are specific to your dataset and use case. While generally available public benchmarks provide a baseline, you should run this framework on your own prompts to get accurate performance metrics. Results can vary significantly based on prompt complexity, token lengths, and network conditions.\n",
    "\n",
    "\n",
    "## Required Dataset Format\n",
    "\n",
    "Your input JSONL file should contain one JSON object per line with the following fields:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text_prompt\": \"Your question or instruction here\",\n",
    "    \"expected_output_tokens\": 50,  // number of tokens expected in output\n",
    "    \"task_type\": \"Text-Generation\",  // currently supports Text-Generation\n",
    "    \"model_id\": \"us.meta.llama3-1-70b-instruct-v1:0\",  // model identifier\n",
    "    \"region\": \"us-west-2\", // region where you want to benchmark model latency metrics\n",
    "    \"inference_profile\": \"optimized\"  // optimization setting \n",
    "}\n",
    "```\n",
    "\n",
    "#### Example entries from the test dataset:\n",
    "```json\n",
    "{\"text_prompt\": \"Summarize the key features of cloud computing in one sentence.\", \"expected_output_tokens\": 50, \"task_type\": \"Text-Generation\", \"model_id\": \"us.meta.llama3-1-70b-instruct-v1:0\", \"region\": \"us-east-2\", \"inference_profile\": \"optimized\"}\n",
    "{\"text_prompt\": \"Explain the concept of machine learning in simple terms.\", \"expected_output_tokens\": 50, \"task_type\": \"Text-Generation\", \"model_id\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\", \"region\": \"us-east-2\", \"inference_profile\": \"optimized\"}\n",
    "{\"text_prompt\": \"Explain the concept of machine learning in simple terms.\", \"expected_output_tokens\": 50, \"task_type\": \"Text-Generation\", \"model_id\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\", \"region\": \"us-east-2\", \"inference_profile\": \"standard\"}\n",
    "```\n",
    "\n",
    "Note: if you configure `\"inference_profile\": \"optimized\"`, you must use `us-east-2` region only because optimized inference is currently only available in `us-east-2` region. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Benchmark Framework Produces\n",
    "Log File:\n",
    "- Created automatically in your working directory\n",
    "- Named latency-benchmarking-experiment-{timestamp}.log\n",
    "- Tracks all API calls, errors, and execution details\n",
    "\n",
    "### Results CSV Files:\n",
    "- Saved in your specified directory\n",
    "- Named invocations_{timestamp}.csv\n",
    "- Contains detailed metrics for each request:\n",
    "- TTFT and completion times\n",
    "- Token counts\n",
    "- API call status\n",
    "- Model details\n",
    "- Task types\n",
    "- Final Analysis\n",
    "\n",
    "### Aggregated metrics by model\n",
    "A detailed performance report containing various statistics like P50 and P90\n",
    "\n",
    "### Prerequisites\n",
    "- AWS credentials with Bedrock access\n",
    "- Input JSONL file containing test prompts in required format\n",
    "- Access to desired AWS region\n",
    "- Enable your selected models hosted on Amazon Bedrock in the region of your choice. Note that model availability can be different in different regions.\n",
    "\n",
    "### Key Features\n",
    "Concurrent API call testing\n",
    "Configurable number of parallel requests\n",
    "Comprehensive error handling and logging\n",
    "Support for different model variants\n",
    "Customizable test scenarios\n",
    "How to Use This Framework\n",
    "Prepare your JSONL file according to the format above\n",
    "Configure parameters in Cell 1 (only cell requiring modification)\n",
    "Run all cells\n",
    "Results will be automatically saved to your specified directory\n",
    "Check the log file for execution details\n",
    "Review the final analysis cell for performance metrics\n",
    "\n",
    "### Data Collection Guidelines\n",
    "\n",
    "The `invocations_per_scenario` parameter determines how many times each prompt is repeated. Since we collect individual metrics for each API call (TTFT, OTPS, token counts), even with `invocations_per_scenario = 10` and 10 different prompts, we get 100 independent observations for the latency benchmarking analysis.\n",
    "\n",
    "For meaningful benchmarking results:\n",
    "- Aim for at least 1000 total observations (can be achieved with fewer repetitions across more prompts); remember higher is better\n",
    "- Run tests for minimum 24 hours and also during your peak traffic times\n",
    "- Align sample distribution with your actual traffic patterns\n",
    "- Use `sleep_between_invocations` to control request rate and costs\n",
    "- Leverage `num_parallel_calls` for concurrent testing\n",
    "\n",
    "> **⚠️ Statistical Note: The Central Limit Theorem applies to our aggregate metrics as we're collecting individual observations for each API call. This means our sample means will approximate a normal distribution as long as we have sufficient total observations, regardless of how we split repetitions across prompts. While a 24-hour collection period helps control for time-of-day variations, extending the collection to multiple days (ideally a two full weeks) will account for day-of-week effects and provide more thorough performance benchmark on your dataset. This is particularly important if your workload patterns vary significantly across different days of the week.\n",
    "\n",
    "## Configuration\n",
    "Set your parameters below. This is the only section that needs modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the environment\n",
    "! pip install --upgrade pandas boto3 numpy==1.26.4 matplotlib seaborn pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the prompt dataset and directory to save the results \n",
    "file_path = \"<your-prompt-dataset-in-above-mentioned-JSONL-format>\"\n",
    "directory = \"<name-of-folder-to-save-results>\"\n",
    "\n",
    "# Configuration to repeat experiment for reliable metrics\n",
    "scenario_config = {\n",
    "    \"sleep_between_invocations\": 60, # in seconds\n",
    "    \"invocations_per_scenario\": 5 # number of times you want to run the same prompt to get more samples - note: this means more cost \n",
    "}\n",
    "\n",
    "# think about how your `num_parallel_calls` value work with `invocations_per_scenario`, right now this means 4 Transactions per minute\n",
    "# Set the number of parallel calls\n",
    "num_parallel_calls = 4\n",
    "\n",
    "# how many times do you want to run the experiment (increase this for longer experiments, helps with more reliable numbers)\n",
    "experiment_counts = 5\n",
    "\n",
    "# Other inference parameters\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 1\n",
    "TOP_K = 250\n",
    "EXPERIMENT_NAME = '<name-and-version-of-your-experiment>' # your custom experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "import random\n",
    "import pprint\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import os\n",
    "import logging\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from typing import List, Dict\n",
    "\n",
    "logging_lock = Lock()\n",
    "os.makedirs(f\"{directory}\", exist_ok=True)\n",
    "os.makedirs(f\"{directory}-analysis\", exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=f\"latency-benchmarking-experiment-{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\", \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a function to get a new boto3 client\n",
    "def get_bedrock_client(region):\n",
    "    config = Config(\n",
    "        retries = dict(\n",
    "            max_attempts = 1\n",
    "        )\n",
    "    )\n",
    "    return boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=region,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "def get_timestamp():\n",
    "    dt = datetime.fromtimestamp(time.time(), tz=pytz.utc)\n",
    "    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def get_body(model_id, file_path, prompt, max_tokens):\n",
    "    body = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                'text': prompt\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    inferenceConfig={\n",
    "        'maxTokens': max_tokens,\n",
    "        'temperature': 0,\n",
    "        'topP': 1\n",
    "    }\n",
    "    return body, inferenceConfig\n",
    "\n",
    "def read_jsonl_files(directory_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_object = json.loads(line.strip())\n",
    "                        all_data.append(json_object)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON in file {filename}: {e}\")\n",
    "    return all_data\n",
    "\n",
    "def post_iteration(scenario_config):\n",
    "    logging.info(f'Sleeping for {scenario_config[\"sleep_between_invocations\"]} seconds.')\n",
    "    time.sleep(scenario_config[\"sleep_between_invocations\"])\n",
    "\n",
    "def benchmark(bedrock, file_path, prompt, latency_inference_profile, max_tokens, model_id=\"\", stream=True, sleep_on_throttling=5):\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "    api_call_status = 'Success'\n",
    "    full_error_message = 'Success'\n",
    "    duration_to_first_byte, duration_to_last_byte = None, None\n",
    "    dt = datetime.fromtimestamp(time.time(), tz=pytz.utc)\n",
    "    job_timestamp_iso = dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    body, inference_config = get_body(model_id, file_path, prompt, max_tokens)\n",
    "    output_token_size, input_token_size = None, None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            response = bedrock.converse_stream(\n",
    "                messages=body,\n",
    "                modelId=model_id,\n",
    "                inferenceConfig=inference_config,\n",
    "                performanceConfig={\n",
    "                        'latency': latency_inference_profile\n",
    "                    }\n",
    "            )\n",
    "            first_byte = None\n",
    "            event_stream = response.get('stream')\n",
    "            for event in event_stream:   \n",
    "                if 'contentBlockDelta' in event:\n",
    "                    chunk = event['contentBlockDelta']\n",
    "                    if chunk:\n",
    "                        if not first_byte:\n",
    "                            first_byte = time.time()  # update the time to first byte\n",
    "                elif 'messageStop' in event:\n",
    "                    stop_reason = event['messageStop'].get('stopReason', 'Unknown')\n",
    "                elif 'metadata' in event:\n",
    "                    metadata = event['metadata']\n",
    "                    if 'usage' in metadata:\n",
    "                        output_token_size = metadata['usage'].get('outputTokens', None)\n",
    "                        input_token_size = metadata['usage'].get('inputTokens', None)\n",
    "            last_byte = time.time()\n",
    "            duration_to_first_byte = round(first_byte - start, 2)\n",
    "            duration_to_last_byte = round(last_byte - start, 2)\n",
    "        except ClientError as err:\n",
    "            full_error_message = err\n",
    "            api_call_status = err.response['Error']['Code']\n",
    "            print(f\"Got Error: {api_call_status}\")\n",
    "            print(f\"Full Error Message: {full_error_message}\")\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    return duration_to_first_byte, duration_to_last_byte, job_timestamp_iso, api_call_status, full_error_message, output_token_size, input_token_size\n",
    "\n",
    "def execute_benchmark(client, scenarios, scenario_config, num_parallel_calls=4, early_break=False):\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    all_invocations = []\n",
    "    \n",
    "    def process_scenario(scenario):\n",
    "        local_client = get_bedrock_client(scenario['region'])\n",
    "        local_invocations = []\n",
    "        file_path = scenario['file_path']\n",
    "        prompt = scenario['prompt']\n",
    "        \n",
    "        for invocation_id in range(scenario_config[\"invocations_per_scenario\"]):\n",
    "            try:\n",
    "                time_to_first_byte, time_to_last_byte, job_timestamp_iso, api_call_status, \\\n",
    "                full_error_message, model_output_tokens, model_input_tokens = benchmark(\n",
    "                    local_client,\n",
    "                    file_path,\n",
    "                    prompt,\n",
    "                    latency_inference_profile=scenario['latency_inference_profile'],\n",
    "                    max_tokens=scenario['configured_output_tokens_for_request'],\n",
    "                    model_id=scenario['model_id'],\n",
    "                    stream=scenario['stream'],\n",
    "                    sleep_on_throttling=scenario_config['sleep_between_invocations']\n",
    "                )\n",
    "\n",
    "                invocation = {\n",
    "                    'time_to_first_byte': time_to_first_byte,\n",
    "                    'time_to_last_byte': time_to_last_byte,\n",
    "                    'job_timestamp_iso': job_timestamp_iso,\n",
    "                    'configured_output_tokens_for_request': scenario['configured_output_tokens_for_request'],\n",
    "                    'model_input_tokens': model_input_tokens,\n",
    "                    'model_output_tokens': model_output_tokens,\n",
    "                    'model': scenario['model_id'],\n",
    "                    'region': scenario['region'],\n",
    "                    'invocation_id': invocation_id,\n",
    "                    'api_call_status': api_call_status,\n",
    "                    'full_error_message': full_error_message,\n",
    "                    'TEMPERATURE': TEMPERATURE,\n",
    "                    'TOP_P': TOP_P,\n",
    "                    'TOP_K': TOP_K,\n",
    "                    'EXPERIMENT_NAME': EXPERIMENT_NAME,\n",
    "                    'task_type': scenario['task_type'],\n",
    "                    'inference_profile': scenario['latency_inference_profile'],\n",
    "                }\n",
    "                local_invocations.append(invocation)\n",
    "                \n",
    "                # Thread-safe logging\n",
    "                with logging_lock:\n",
    "                    logging.info(f'Invocation: {invocation}')\n",
    "                \n",
    "                post_iteration(scenario_config=scenario_config)\n",
    "                \n",
    "            except Exception as e:\n",
    "                with logging_lock:\n",
    "                    logging.error(f\"Error while processing scenario: {scenario['model_id']}. Error: {e}\")\n",
    "                \n",
    "        return local_invocations\n",
    "\n",
    "    # Execute scenarios in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_parallel_calls) as executor:\n",
    "        # Submit all scenarios and store futures\n",
    "        future_to_scenario = {executor.submit(process_scenario, scenario): scenario \n",
    "                            for scenario in scenarios}\n",
    "        \n",
    "        # Print initial state\n",
    "        print(f\"Total scenarios submitted: {len(future_to_scenario)}\")\n",
    "        print(f\"Number of parallel workers: {num_parallel_calls}\")\n",
    "        \n",
    "        # Monitor futures as they complete\n",
    "        start_time = time.time()\n",
    "        running_futures = set()\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_scenario):\n",
    "            scenario = future_to_scenario[future]\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_invocations.extend(result)\n",
    "            except Exception as e:\n",
    "                with logging_lock:\n",
    "                    logging.error(f\"Scenario failed: {e}\")\n",
    "\n",
    "        return all_invocations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    use_cases_scenarios = []\n",
    "\n",
    "    # Read the JSONL file and process each line\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            file = json.loads(line.strip())\n",
    "            prompt = file.get('text_prompt')\n",
    "            task_type = file.get('task_type')\n",
    "            model_id = file.get('model_id')\n",
    "            region = file.get('region')\n",
    "            latency_inference_profile = file.get('inference_profile', 'optimized')\n",
    "\n",
    "            out_tokens = file.get('expected_output_tokens', 100)\n",
    "            use_cases_scenarios.append({\n",
    "                \"file_path\": file_path,\n",
    "                \"configured_output_tokens_for_request\": out_tokens,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": True,\n",
    "                \"model_id\": model_id,\n",
    "                \"region\": region,\n",
    "                \"task_type\": task_type,\n",
    "                \"latency_inference_profile\": latency_inference_profile\n",
    "            })\n",
    "\n",
    "    # Main loop\n",
    "    run_count = 1\n",
    "    while run_count <= experiment_counts:\n",
    "        selected_scenarios = random.sample(\n",
    "            use_cases_scenarios, \n",
    "            k=len(use_cases_scenarios) // 1\n",
    "        )\n",
    "\n",
    "        with logging_lock:\n",
    "            logging.info(f\"{len(selected_scenarios)} scenarios x {scenario_config['invocations_per_scenario']} invocations = {len(selected_scenarios) * scenario_config['invocations_per_scenario']} total invocations\")\n",
    "        \n",
    "        logging.info(f\"Running iteration {run_count}\")\n",
    "        \n",
    "        # Create a new client for the main thread\n",
    "        config = Config(\n",
    "            retries = dict(\n",
    "                max_attempts = 1\n",
    "            )\n",
    "        )\n",
    "        client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=region,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Run the scenarios and measure times\n",
    "        invocations = execute_benchmark(\n",
    "            client, \n",
    "            selected_scenarios, \n",
    "            scenario_config, \n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "            early_break=False\n",
    "        )\n",
    "\n",
    "        # Convert the invocations list to a pandas DataFrame\n",
    "        df = pd.DataFrame(invocations)\n",
    "        df['timestamp'] = pd.Timestamp.now()\n",
    "        df['run_count'] = run_count\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        output_file = f\"{directory}/invocations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        with logging_lock:\n",
    "            logging.info(f\"Results written to {output_file}\")\n",
    "            logging.info(f\"Completed run {run_count} of 500\")\n",
    "\n",
    "        run_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "This section analyzes the collected latency metrics grouped by model ID. The analysis includes:\n",
    "\n",
    "1. Time to First Token (TTFT)\n",
    "   - Average\n",
    "   - P50 (median)\n",
    "   - P90\n",
    "\n",
    "2. Output Tokens per Second (OTPS)\n",
    "   - Average\n",
    "   - P50 (median)\n",
    "   - P90\n",
    "\n",
    "Note: Results may vary based on network conditions, prompt length, and other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def combine_csv_files(directory):\n",
    "    \"\"\"Combine all CSV files in the directory into a single DataFrame.\"\"\"\n",
    "    all_files = glob.glob(os.path.join(directory, \"invocations_*.csv\"))\n",
    "    df_list = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename)\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "def calculate_metrics(df, group_columns):\n",
    "    \"\"\"Calculate latency metrics grouped by model, region, and inference profile.\"\"\"\n",
    "    metrics = df.groupby(group_columns).agg({\n",
    "        'time_to_first_byte': ['count', 'mean', 'median', \n",
    "                              lambda x: x.quantile(0.9), \n",
    "                              lambda x: x.std()],\n",
    "        'model_input_tokens': ['mean'],\n",
    "        'model_output_tokens': ['mean'],\n",
    "        'time_to_last_byte': ['mean', 'median', \n",
    "                             lambda x: x.quantile(0.9)]\n",
    "    }).round(3)\n",
    "    \n",
    "    metrics.columns = ['sample_size', 'TTFT_mean', 'TTFT_p50', 'TTFT_p90', 'TTFT_std',\n",
    "                      'avg_input_tokens',\n",
    "                      'avg_output_tokens',\n",
    "                      'total_time_mean', 'total_time_p50', 'total_time_p90']\n",
    "    \n",
    "    df['OTPS'] = df['model_output_tokens'] / df['time_to_last_byte']\n",
    "    otps_metrics = df.groupby(group_columns)['OTPS'].agg(['mean', 'median', \n",
    "                                                         lambda x: x.quantile(0.9),\n",
    "                                                         lambda x: x.std()]).round(3)\n",
    "    otps_metrics.columns = ['OTPS_mean', 'OTPS_p50', 'OTPS_p90', 'OTPS_std']\n",
    "    \n",
    "    metrics = pd.concat([metrics, otps_metrics], axis=1)\n",
    "    return metrics\n",
    "\n",
    "def create_performance_summary_tables(df, metrics, pdf):\n",
    "    \"\"\"Create performance summary tables split across multiple pages if needed.\"\"\"\n",
    "    MAX_MODELS_PER_PAGE = 4\n",
    "    \n",
    "    # Use all metrics from the metrics DataFrame\n",
    "    METRICS_TO_SHOW = metrics.columns.tolist()\n",
    "    \n",
    "    # Get unique models\n",
    "    models = metrics.index.get_level_values('model').unique()\n",
    "    model_chunks = [models[i:i + MAX_MODELS_PER_PAGE] \n",
    "                   for i in range(0, len(models), MAX_MODELS_PER_PAGE)]\n",
    "    \n",
    "    for page_num, models_subset in enumerate(model_chunks, 1):\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        data_rows = []\n",
    "        row_labels = []\n",
    "        col_labels = []\n",
    "        valid_columns = []\n",
    "\n",
    "        # Get actual existing combinations from the metrics DataFrame\n",
    "        for model in models_subset:\n",
    "            model_display_name = model.split('.')[-1]\n",
    "            try:\n",
    "                model_data = metrics.xs(model, level='model')\n",
    "                for (region, profile) in model_data.index:\n",
    "                    if not model_data.loc[(region, profile)].isna().all():\n",
    "                        col_labels.append(f\"{model_display_name}\\n{region}\\n{profile}\")\n",
    "                        valid_columns.append((model, region, profile))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        # Create row labels and data only for valid combinations\n",
    "        for metric in METRICS_TO_SHOW:\n",
    "            row_labels.append(metric)\n",
    "            row_data = []\n",
    "            \n",
    "            for model, region, profile in valid_columns:\n",
    "                try:\n",
    "                    value = metrics.loc[(model, region, profile), metric]\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        if metric == 'sample_size':\n",
    "                            row_data.append(f\"{value:.0f}\")\n",
    "                        else:\n",
    "                            row_data.append(f\"{value:.2f}\")\n",
    "                    else:\n",
    "                        row_data.append(str(value))\n",
    "                except KeyError:\n",
    "                    continue  # Skip if combination doesn't exist\n",
    "            \n",
    "            data_rows.append(row_data)\n",
    "\n",
    "        # Create table only if there are valid columns\n",
    "        if valid_columns:\n",
    "            table = ax.table(cellText=data_rows,\n",
    "                           colLabels=col_labels,\n",
    "                           rowLabels=row_labels,\n",
    "                           cellLoc='center',\n",
    "                           loc='center',\n",
    "                           bbox=[0.05, 0.05, 0.95, 0.95])\n",
    "            \n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(8)\n",
    "            \n",
    "            for k, cell in table._cells.items():\n",
    "                if k[0] == 0:  # Header row\n",
    "                    cell.set_height(0.15)\n",
    "                    cell.set_text_props(ha='center', va='center')\n",
    "                    cell.set_fontsize(7)\n",
    "                    cell.set_text_props(weight='bold')\n",
    "                \n",
    "                if k[1] == -1:  # Row headers (metrics names)\n",
    "                    cell.set_width(0.20)\n",
    "                    cell.set_text_props(ha='left')\n",
    "                else:\n",
    "                    cell.set_width(0.80 / len(valid_columns))\n",
    "            \n",
    "            plt.title(f'Performance Metrics Summary (Page {page_num} of {len(model_chunks)})', pad=20)\n",
    "            pdf.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def plot_model_distributions(df, metric, metric_name, pdf):\n",
    "    \"\"\"Create distribution plots grouped by model, region, and inference profile.\"\"\"\n",
    "    model_profiles = df.groupby(['model', 'region', 'inference_profile']).size().reset_index()\n",
    "    n_combinations = len(model_profiles)\n",
    "    \n",
    "    n_cols = min(2, n_combinations)\n",
    "    n_rows = (n_combinations + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 6*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.reshape(-1, 1) if n_cols == 1 else axes.reshape(1, -1)\n",
    "    \n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    for idx, (_, row) in enumerate(model_profiles.iterrows()):\n",
    "        model = row['model']\n",
    "        region = row['region']\n",
    "        profile = row['inference_profile']\n",
    "        ax = axes_flat[idx]\n",
    "        \n",
    "        mask = (df['model'] == model) & (df['region'] == region) & (df['inference_profile'] == profile)\n",
    "        data = df[mask][metric]\n",
    "        \n",
    "        sns.histplot(data=data, kde=True, bins=30, ax=ax)\n",
    "        \n",
    "        ax.axvline(data.mean(), color='r', linestyle='--', \n",
    "                  label=f'Mean: {data.mean():.2f}')\n",
    "        ax.axvline(data.median(), color='g', linestyle='--', \n",
    "                  label=f'Median: {data.median():.2f}')\n",
    "        ax.axvline(data.quantile(0.9), color='b', linestyle='--', \n",
    "                  label=f'P90: {data.quantile(0.9):.2f}')\n",
    "        \n",
    "        model_display_name = model.split('.')[-1]\n",
    "        ax.set_title(f'{model_display_name}\\n{region}\\n{profile}')\n",
    "        ax.set_xlabel(metric_name)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.legend(fontsize='small')\n",
    "    \n",
    "    for idx in range(len(model_profiles), len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'{metric_name} Distribution by Model, Region, and Inference Profile')\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_comparison(df, metric, metric_name, pdf):\n",
    "    \"\"\"Create box plot comparing models by inference profile.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    df = df.copy()\n",
    "    # Create combined model-region display name\n",
    "    df['model_display'] = df.apply(lambda x: f\"{x['model'].split('.')[-1]}\\n({x['region']})\", axis=1)\n",
    "    \n",
    "    # Create box plot with inference_profile as hue\n",
    "    ax = sns.boxplot(data=df, x='model_display', y=metric, hue='inference_profile')\n",
    "    \n",
    "    q1 = df[metric].quantile(0.25)\n",
    "    q3 = df[metric].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    upper_whisker = q3 + 1.5 * iqr\n",
    "    \n",
    "    plt.ylim(0, upper_whisker * 1.2)\n",
    "    plt.title(f'{metric_name} Comparison Across Models and Optimized-Inference')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Profile')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "def check_and_create_new_page(y_pos, pdf, min_space_needed=0.2):\n",
    "    \"\"\"\n",
    "    Check if we need a new page and create one if necessary.\n",
    "    Returns: new y_position (either on same or new page)\n",
    "    \"\"\"\n",
    "    if y_pos < min_space_needed:\n",
    "        pdf.savefig(bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Create new page\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        plt.axis('off')\n",
    "        return 0.95\n",
    "    return y_pos\n",
    "\n",
    "def analyze_latency_metrics(directory):\n",
    "    \"\"\"Main analysis function with PDF report generation.\"\"\"\n",
    "    # Turn off interactive plotting\n",
    "    plt.ioff()\n",
    "    # Close any existing plots\n",
    "    plt.close('all')\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    pdf_file = os.path.join(f\"{directory}-analysis\", f'latency_analysis_report_{timestamp}.pdf')\n",
    "    \n",
    "    with PdfPages(pdf_file) as pdf:\n",
    "        # Create title page\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Main title\n",
    "        plt.text(0.5, 0.8, 'Latency Analysis Report',\n",
    "                ha='center', va='center', size=24, weight='bold')\n",
    "        \n",
    "        # Timestamp\n",
    "        plt.text(0.5, 0.7, f'Generated on: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "                ha='center', va='center', size=12, style='italic', color='#666666')\n",
    "        \n",
    "        # Metrics section title\n",
    "        plt.text(0.5, 0.5, 'Key Metrics',\n",
    "                ha='center', va='center', size=18, weight='bold')\n",
    "        \n",
    "        # TTFT section\n",
    "        plt.text(0.5, 0.4, 'TTFT (Time to First Token)',\n",
    "                ha='center', va='center', size=14, weight='bold', color='#2E5A88')\n",
    "        plt.text(0.5, 0.35, 'Lower values indicate better performance',\n",
    "                ha='center', va='center', size=12, style='italic', color='#666666')\n",
    "        \n",
    "        # OTPS section\n",
    "        plt.text(0.5, 0.25, 'OTPS (Output Tokens Per Second)',\n",
    "                ha='center', va='center', size=14, weight='bold', color='#2E5A88')\n",
    "        plt.text(0.5, 0.2, 'Higher values indicate better performance',\n",
    "                ha='center', va='center', size=12, style='italic', color='#666666')\n",
    "        \n",
    "        pdf.savefig(bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Loading data from:\", directory)\n",
    "        df = combine_csv_files(directory)\n",
    "        \n",
    "        # Count error requests\n",
    "        errored_requests = df[df['api_call_status'] != 'Success']\n",
    "        errored_count = len(errored_requests)\n",
    "\n",
    "        # Count throttled requests\n",
    "        throttled_requests = df[df['api_call_status'] == 'ThrottlingException']\n",
    "        throttled_count = len(throttled_requests)\n",
    "        \n",
    "        # Remove error requests from analysis\n",
    "        df = df[df['api_call_status'] == 'Success']\n",
    "        \n",
    "        # Calculate OTPS for valid requests\n",
    "        df['OTPS'] = df['model_output_tokens'] / df['time_to_last_byte']\n",
    "        \n",
    "        # Summary statistics page\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        plt.axis('off')\n",
    "               \n",
    "        # Section 1: API Statistics\n",
    "        plt.text(0.1, 0.95, 'Summary Statistics', size=18, weight='bold')\n",
    "        plt.text(0.1, 0.90, f\"Total API calls: {len(df) + errored_count}\", size=12)\n",
    "        plt.text(0.1, 0.86, f\"Successful calls: {len(df)}\", size=12)\n",
    "        plt.text(0.1, 0.82, f\"Errors calls: {errored_count} ({(errored_count/(len(df) + errored_count)*100):.1f}%)\", \n",
    "                size=12, color='#666666')\n",
    "        plt.text(0.1, 0.78, f\"Throttled calls: {throttled_count} ({(throttled_count/(len(df) + throttled_count)*100):.1f}%)\", \n",
    "                size=12, color='#666666')\n",
    "\n",
    "        # Token Statistics section\n",
    "        plt.text(0.1, 0.70, 'Token Statistics', size=18, weight='bold')\n",
    "        plt.text(0.1, 0.65, f\"Average Input Tokens: {df['model_input_tokens'].mean():.1f}\", size=12)\n",
    "        plt.text(0.1, 0.61, f\"Max Input Tokens: {df['model_input_tokens'].max():.0f}\", size=12)\n",
    "        plt.text(0.1, 0.57, f\"Average Output Tokens: {df['model_output_tokens'].mean():.1f}\", size=12)\n",
    "        plt.text(0.1, 0.53, f\"Max Output Tokens: {df['model_output_tokens'].max():.0f}\", size=12)\n",
    "\n",
    "        # Section 2: Model Information\n",
    "        plt.text(0.1, 0.45, 'Model Information', size=18, weight='bold')\n",
    "        plt.text(0.1, 0.40, f\"Number of unique models: {df['model'].nunique()}\", size=12)\n",
    "        plt.text(0.1, 0.36, \"Models:\", size=12)\n",
    "        \n",
    "        y_pos = 0.32\n",
    "        for model in df['model'].unique():\n",
    "            y_pos = check_and_create_new_page(y_pos, pdf)\n",
    "            model_display_name = model\n",
    "            plt.text(0.15, y_pos, f\"• {model_display_name}\", size=12, color='#2E5A88')\n",
    "            y_pos -= 0.04\n",
    "\n",
    "        # Section 3: Inference Profiles\n",
    "        if 'inference_profile' in df.columns:\n",
    "            y_pos = check_and_create_new_page(y_pos, pdf)\n",
    "            y_pos -= 0.02  # Space between sections\n",
    "            plt.text(0.1, y_pos, 'Inference Profiles', size=18, weight='bold')\n",
    "            y_pos -= 0.05\n",
    "            \n",
    "            for profile in df['inference_profile'].unique():\n",
    "                y_pos = check_and_create_new_page(y_pos, pdf)\n",
    "                plt.text(0.15, y_pos, f\"• {profile}\", size=12, color='#2E5A88')\n",
    "                y_pos -= 0.04\n",
    "\n",
    "        # Section 4: Sample Distribution\n",
    "        y_pos = check_and_create_new_page(y_pos, pdf)\n",
    "        y_pos -= 0.02\n",
    "        plt.text(0.1, y_pos, 'Sample Distribution', size=18, weight='bold')\n",
    "        y_pos -= 0.05\n",
    "\n",
    "        if 'inference_profile' in df.columns:\n",
    "            model_profile_counts = df.groupby(['model', 'inference_profile', 'region']).size()\n",
    "            for (model, profile, region), count in model_profile_counts.items():\n",
    "                y_pos = check_and_create_new_page(y_pos, pdf)\n",
    "                model_display_name = model.split('.')[-1]\n",
    "                plt.text(0.15, y_pos, \n",
    "                        f\"• {model_display_name} in {region} with ({profile}) inference: {count} samples\",\n",
    "                        size=12, color='#2E5A88')\n",
    "                y_pos -= 0.04\n",
    "\n",
    "        pdf.savefig(bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # metrics = calculate_metrics(df, ['model', 'inference_profile'])\n",
    "        # Basic metrics table\n",
    "        metrics = calculate_metrics(df, ['model', 'region', 'inference_profile'])\n",
    "        create_performance_summary_tables(df, metrics, pdf)\n",
    "        \n",
    "        # Distribution plots\n",
    "        plot_model_distributions(df, 'time_to_first_byte', 'Time to First Token (seconds)', pdf)\n",
    "        plot_model_distributions(df, 'OTPS', 'Output Tokens Per Second', pdf)\n",
    "        \n",
    "        # Model comparisons\n",
    "        plot_model_comparison(df, 'time_to_first_byte', 'TTFT', pdf)\n",
    "        plot_model_comparison(df, 'OTPS', 'OTPS', pdf)\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        csv_file = os.path.join(f\"{directory}-analysis\", f'analysis_summary_{timestamp}.csv')\n",
    "        metrics.to_csv(csv_file)\n",
    "        \n",
    "        print(f\"\\nAnalysis complete!\")\n",
    "        print(f\"PDF report saved to: {pdf_file}\")\n",
    "        print(f\"CSV summary saved to: {csv_file}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_latency_metrics(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
