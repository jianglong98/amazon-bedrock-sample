{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Knowledge Base Evaluation Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock Knowledge Base Evaluation provides a comprehensive solution for assessing RAG (Retrieval-Augmented Generation) applications. This guide demonstrates how to evaluate both retrieval and generation components of your RAG system using Amazon Bedrock APIs.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Setting up evaluation configurations\n",
    "- Creating retrieval only evaluation jobs\n",
    "- Creating retrieval with generation evaluation jobs\n",
    "- Monitoring evaluation progress\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket with CORS enabled for storing evaluation data\n",
    "- A created and synced Amazon Bedrock Knowledge Base\n",
    "- An IAM role with necessary permissions for S3 and Bedrock\n",
    "- To complete these prerequisites, check the how to steps avaialble [here](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-evaluation-prereq.html)\n",
    "\n",
    "> **Important**: Make sure that your knowledge base is synced and ready before starting any evaluation job.\n",
    "\n",
    "## Dataset Format\n",
    "\n",
    "The evaluation data must follow specific JSONL formats based on the type of evaluation:\n",
    "\n",
    "### Retrieval-only Evaluation Format\n",
    "```json\n",
    "{\n",
    "    \"conversationTurns\": [{\n",
    "        \"referenceContexts\": [{\n",
    "            \"content\": [{\n",
    "                \"text\": \"Reference context for evaluation\"\n",
    "            }]\n",
    "        }],\n",
    "        \"prompt\": {\n",
    "            \"content\": [{\n",
    "                \"text\": \"Your prompt here\"\n",
    "            }]\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "```\n",
    "\n",
    "### Retrieval and Generation Evaluation Format\n",
    "```json\n",
    "{\n",
    "    \"conversationTurns\": [{\n",
    "        \"referenceResponses\": [{\n",
    "            \"content\": [{\n",
    "                \"text\": \"Reference response for evaluation\"\n",
    "            }]\n",
    "        }],\n",
    "        \"prompt\": {\n",
    "            \"content\": [{\n",
    "                \"text\": \"Your prompt here\"\n",
    "            }]\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "```\n",
    "\n",
    "## Dataset Requirements\n",
    "\n",
    "### Job Requirements\n",
    "- Maximum 1000 prompts per evaluation job\n",
    "- Each line in the JSONL file must be a complete prompt\n",
    "\n",
    "### File Requirements\n",
    "- File must use JSONL format with `.jsonl` extension\n",
    "- Each line must be a valid JSON object\n",
    "- File must be stored in an S3 bucket with CORS enabled\n",
    "\n",
    "### Data Structure Requirements\n",
    "For Retrieval-only Evaluation:\n",
    "- Must include `referenceContexts` as shown in the format above\n",
    "- Each prompt must follow the specified JSON structure\n",
    "\n",
    "For Retrieval and Generation Evaluation:\n",
    "- Optional `referenceResponses` as shown in the format above\n",
    "- Must follow the specified JSON structure\n",
    "\n",
    "> **Note**: When preparing your dataset, consider your evaluation objectives and make sure that your prompts and reference data align with your assessment goals. \n",
    "\n",
    "## Implementation\n",
    "\n",
    "First, let's set up our configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate unique name for the job\n",
    "job_name = f\"kb-evaluation-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "knowledge_base_id = \"<YOUR_KB_ID>\"\n",
    "evaluator_model = \"mistral.mistral-large-2402-v1:0\"\n",
    "generator_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "\n",
    "# Specify S3 locations\n",
    "input_data = \"s3://<YOUR_BUCKET>/evaluation_data/input.jsonl\"\n",
    "output_path = \"s3://<YOUR_BUCKET>/evaluation_output/\"\n",
    "\n",
    "# Configure retrieval settings\n",
    "num_results = 5\n",
    "search_type = \"HYBRID\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Retrieval-only Evaluation Job\n",
    "\n",
    "This configuration focuses on assessing the quality of retrieved contexts. Available metrics for retrieval evaluation:\n",
    "- `Builtin.ContextRelevance`: Assesses how relevant the retrieved contexts are to the query\n",
    "- `Builtin.ContextCoverage`: Measures how well the retrieved contexts cover the information needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_job = bedrock_client.create_evaluation_job(\n",
    "    jobName=job_name,\n",
    "    jobDescription=\"Evaluate retrieval performance\",\n",
    "    roleArn=role_arn,\n",
    "    applicationType=\"RagEvaluation\",\n",
    "    inferenceConfig={\n",
    "        \"ragConfigs\": [{\n",
    "            \"knowledgeBaseConfig\": {\n",
    "                \"retrieveConfig\": {\n",
    "                    \"knowledgeBaseId\": knowledge_base_id,\n",
    "                    \"knowledgeBaseRetrievalConfiguration\": {\n",
    "                        \"vectorSearchConfiguration\": {\n",
    "                            \"numberOfResults\": num_results,\n",
    "                            \"overrideSearchType\": search_type\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"automated\": {\n",
    "            \"datasetMetricConfigs\": [{\n",
    "                \"taskType\": \"Custom\",\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"RagDataset\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": input_data\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                    \"Builtin.ContextRelevance\",\n",
    "                    \"Builtin.ContextCoverage\"\n",
    "                ]\n",
    "            }],\n",
    "            \"evaluatorModelConfig\": {\n",
    "                \"bedrockEvaluatorModels\": [{\n",
    "                    \"modelIdentifier\": evaluator_model\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Retrieval and Generation Evaluation Job\n",
    "\n",
    "This configuration evaluates both retrieval and response generation. Available metrics for this evaluation:\n",
    "- `Builtin.Correctness`: Evaluates factual accuracy of generated responses\n",
    "- `Builtin.Completeness`: Assesses if all relevant information is included\n",
    "- `Builtin.Helpfulness`: Measures how useful the response is\n",
    "- `Builtin.LogicalCoherence`: Evaluates response structure and flow\n",
    "- `Builtin.Faithfulness`: Checks for hallucinations or made-up information\n",
    "- `Builtin.Harmfulness`: Detects harmful content\n",
    "- `Builtin.Stereotyping`: Identifies biased or stereotypical responses\n",
    "- `Builtin.Refusal`: Evaluates appropriate refusal of problematic requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "job_name_rg = f\"kb-evaluation-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "    jobName=job_name_rg,\n",
    "    jobDescription=\"Evaluate retrieval and generation\",\n",
    "    roleArn=role_arn,\n",
    "    applicationType=\"RagEvaluation\",\n",
    "    inferenceConfig={\n",
    "        \"ragConfigs\": [{\n",
    "            \"knowledgeBaseConfig\": {\n",
    "                \"retrieveAndGenerateConfig\": {\n",
    "                    \"type\": \"KNOWLEDGE_BASE\",\n",
    "                    \"knowledgeBaseConfiguration\": {\n",
    "                        \"knowledgeBaseId\": knowledge_base_id,\n",
    "                        \"modelArn\": generator_model,\n",
    "                        \"retrievalConfiguration\": {\n",
    "                            \"vectorSearchConfiguration\": {\n",
    "                                \"numberOfResults\": num_results,\n",
    "                                \"overrideSearchType\": search_type\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"automated\": {\n",
    "            \"datasetMetricConfigs\": [{\n",
    "                \"taskType\": \"Custom\",\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"RagDataset\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": input_data\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                    \"Builtin.Correctness\",\n",
    "                    \"Builtin.Completeness\",\n",
    "                    \"Builtin.Helpfulness\",\n",
    "                    \"Builtin.LogicalCoherence\",\n",
    "                    \"Builtin.Faithfulness\"\n",
    "                ]\n",
    "            }],\n",
    "            \"evaluatorModelConfig\": {\n",
    "                \"bedrockEvaluatorModels\": [{\n",
    "                    \"modelIdentifier\": evaluator_model\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Job Progress\n",
    "\n",
    "Track the status of your evaluation job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = retrieval_job['jobArn']  # or retrieve_generate_job['jobArn']\n",
    "\n",
    "# Check job status\n",
    "response = bedrock_client.get_evaluation_job(\n",
    "    jobIdentifier=evaluation_job_arn \n",
    ")\n",
    "print(f\"Job Status: {response['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this guide, we've walked through the process of implementing Knowledge Base Evaluation using Amazon Bedrock. The feature enables organizations to:\n",
    "- Assess AI model outputs across various tasks and contexts\n",
    "- Evaluate multiple dimensions of AI performance simultaneously\n",
    "- Systematically assess both retrieval and generation quality in RAG systems\n",
    "- Scale evaluations across thousands of responses while maintaining quality standards\n",
    "\n",
    "Remember to follow the best practices outlined above to ensure effective evaluation of your RAG applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
