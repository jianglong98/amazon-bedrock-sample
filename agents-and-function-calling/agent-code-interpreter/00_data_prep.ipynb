{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7215eb-5290-4dde-98ac-0d9342b5367b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This repository demonstrates how to set up, use, and test an Amazon Bedrock Agent with Code Interpreter capabilities. The project is divided into three Jupyter notebooks, each focusing on a specific aspect of the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0814b-b2a4-4822-b8cb-3e867c31f6aa",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "This is the first notebook in the series to demonstrates how to set up and use an Amazon Bedrock Agent with Code Interpreter capabilities.\n",
    "\n",
    "In this notebook we process open souce NYC Taxi and Limousine data to be used by our Amazon Bedrock Agent later\n",
    "#### NYC TLC Trip Record Data\n",
    "\n",
    "- **Source**: [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "- **Content**: Yellow and green taxi trip records (pickup/dropoff times, locations, fares, etc.)\n",
    "\n",
    "#### Process\n",
    "1. Download Parquet file for target date\n",
    "2. Convert to CSV, reduce to <100MB\n",
    "3. Upload to S3 for agent use\n",
    "\n",
    "Note: Ensure S3 upload permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc898cbd-6838-448f-b9de-d5685e43bd07",
   "metadata": {},
   "source": [
    "<h2>Prerequisites</h2>\n",
    "\n",
    "Apart from the libraries that we will be installing, this notebook requires permissions to:\n",
    "\n",
    "<ul>\n",
    "<li>access Amazon Bedrock</li>\n",
    "</ul>\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "<ul>\n",
    "<li>AmazonBedrockFullAccess</li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to enable `Anthropic Claude 3.5 Sonnet` model access in Amazon Bedrock Console, as the later notebook will use Anthropic Claude 3.5 Sonnet model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69799993-d7a8-47ec-bca1-fd0e93372203",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We need to import the necessary Python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131261dd-291b-414a-bbd0-d64f9415a760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c87190-10fc-4ed5-9484-0f263d90a330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30aaf01-7fb8-4cb2-9604-f445e0cadb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get the default S3 bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"Default S3 bucket: {default_bucket}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60217b71-57a7-42f5-b73c-e460f06a47d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "CSV_DATA_FILE: str = 'nyc_taxi_subset.csv'\n",
    "# Bucket and prefix name where this csv file will be uploaded and used as S3 source by code interpreter\n",
    "S3_BUCKET_NAME: str = default_bucket\n",
    "PREFIX: str = 'code-interpreter-demo-data'\n",
    "# This is the size of the file that will be uploaded to s3 and used by the agent (in MB)\n",
    "DATASET_FILE_SIZE: float = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6699d1b-8fc4-4e6c-9092-5149772594c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_nyc_taxi_data(start_date, end_date, data_types):\n",
    "    base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "    \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        for data_type in data_types:\n",
    "            file_name = f\"{data_type}_tripdata_{current_date.strftime('%Y-%m')}.parquet\"\n",
    "            url = base_url + file_name\n",
    "            \n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            \n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                output_dir = f\"nyc_taxi_data/{data_type}/{current_date.year}\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                with open(os.path.join(output_dir, file_name), 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Successfully downloaded {file_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "        \n",
    "        current_date += timedelta(days=32)\n",
    "        current_date = current_date.replace(day=1)\n",
    "\n",
    "# Set the date range for which you want to download data\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 1, 31)\n",
    "\n",
    "# Specify the types of data you want to download\n",
    "data_types = ['yellow']\n",
    "\n",
    "# Download the data\n",
    "download_nyc_taxi_data(start_date, end_date, data_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d59b3-60c5-4608-b8bb-b420f5a137b9",
   "metadata": {},
   "source": [
    "## Prepare the large data and send it to S3 to be used by the agent\n",
    "\n",
    "Now, we will prepare the data and upload it to S3. This is the new york taxi dataset. S3 allows for larger files (100MB) to be used by the agent for code interpretation, so we will upload a CSV file that is `99.67 MB` in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f331c8-ad01-4cb6-8cba-b6a509c5b38d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the parquet file into a pandas DataFrame\n",
    "nyc_taxi_df = pd.read_parquet(\"./nyc_taxi_data/yellow/2024/yellow_tripdata_2024-01.parquet\")\n",
    "nyc_taxi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefef7c-de0e-4f8a-abfc-227875626634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_with_size_limit(df: pd.DataFrame, \n",
    "                              output_file: str, \n",
    "                              size_limit_mb: float = 99):\n",
    "    \"\"\"\n",
    "    This function writes a pandas DataFrame to a CSV file with a given limit\n",
    "    in size (in MB)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chunk_size: int = 10000 \n",
    "        total_rows = len(df)\n",
    "        start_index: int = 0\n",
    "        while start_index < total_rows:\n",
    "            # Write a chunk of data\n",
    "            end_index = min(start_index + chunk_size, total_rows)\n",
    "            chunk = df.iloc[start_index:end_index]\n",
    "            \n",
    "            mode = 'w' if start_index == 0 else 'a'\n",
    "            chunk.to_csv(output_file, mode=mode, header=(start_index == 0), index=False)\n",
    "            \n",
    "            # Check file size\n",
    "            current_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "            \n",
    "            if current_size_mb >= size_limit_mb:\n",
    "                logger.info(f\"Reached size limit. Current file size: {current_size_mb:.2f} MB\")\n",
    "                break\n",
    "            \n",
    "            start_index = end_index\n",
    "            \n",
    "        final_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        logger.info(f\"Final file size: {final_size_mb:.2f} MB\")\n",
    "        logger.info(f\"Rows written: {end_index} out of {total_rows}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while writing to the csv file with the limit of {size_limit_mb}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7229e8c-dc3c-4a05-93f3-b31a7330d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_with_size_limit(nyc_taxi_df, CSV_DATA_FILE, size_limit_mb=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036aa4c7-15d9-4624-b969-731f8617e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = os.path.getsize(CSV_DATA_FILE)\n",
    "# Convert to megabytes\n",
    "size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "logger.info(f\"Size of the {CSV_DATA_FILE} is: {size_in_mb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6663aeb-c972-4e4d-8246-13100827d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(CSV_DATA_FILE, S3_BUCKET_NAME, f\"{PREFIX}/{os.path.basename(CSV_DATA_FILE)}\")\n",
    "s3_uri: str = f\"s3://{S3_BUCKET_NAME}/{PREFIX}/{os.path.basename(CSV_DATA_FILE)}\"\n",
    "logger.info(f\"File uploaded successfully. S3 URI: {s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a597a3-0acc-482b-8b59-f44595f7e6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the S3 URI to a text file\n",
    "with open('s3_uri.txt', 'w') as f:\n",
    "    f.write(s3_uri)\n",
    "\n",
    "print(\"S3 URI has been written to s3_uri.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
