{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd65e66-7d04-4f08-bd89-bf1027c5179a",
   "metadata": {},
   "source": [
    "# How to do function calling using InvokeModel API and model-specific prompting \n",
    "\n",
    "This notebook demonstrates how we can use the `InvokeModel API` with external functions to support tool calling. \n",
    "\n",
    "Although `Converse` and `ConverseStream` provide a unified structured text action for simplifying the invocations to Amazon Bedrock LLMs, along with the use of `Tool` for function calling, some customers may choose to call `InvokeModel` or `InvokeModelWithResponseStream` supplying model-specific parameters and prompts. \n",
    "\n",
    "Most differentiated real-world applications require access to real-time data and the ability to interact with it. On their own, models do not have the ability to call external functions or APIs to bridge this gap. To solve this, function calling lets developers define a set of tools (external functions) the model has access to and, defines instructions the model uses to return a structured output that can be used to call the function. A tool definition includes its name, description and input schema. The model can be given a certain level of freedom when choosing to answer user requests using a set of tools. \n",
    "\n",
    "We cover the prompting components required to enable a model to call the correct tools based on a given input request.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Tool calling with Anthropic Claude 3.5 Sonnet** We demonstrate how to define a single tool. In our case, for simulating a stock ticker symbol lookup tool `get_ticker_symbol` and allow the model to call this tool to return a a ticker symbol.\n",
    "- **Tool calling with Meta Llama 3.1** We modify the prompts to fit Meta's suggested prompt format.\n",
    "- **Tool calling with Mistral AI Large** We modify the prompts to fit Mistral's suggested prompt format.\n",
    "- **Tool calling with Cohere Command R+** We modify the prompts to fit Cohere's suggested prompt format.\n",
    "\n",
    "## Tool calling with Anthropic Claude 3.5 Sonnet\n",
    "\n",
    "We set our tools and functions through Python functions.\n",
    "\n",
    "We start by defining a tool for simulating a stock ticker symbol lookup tool (`get_ticker_symbol`). Note in our example we're just returning a constant ticker symbol for a select group of companies to illustrate the concept, but you could make it fully functional by connecting it to any stock or finance API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "431698d7-ae6d-4726-aad5-3b5b657236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 --quiet\n",
    "!pip install botocore --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install lxml --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c9bb9-f3b0-4e7b-9309-f730688a583c",
   "metadata": {},
   "source": [
    "This first example leverages Claude Sonnet 3.5 in the `us-west-2` region. Later, we continue with implementations using various other models available in Amazon Bedrock. The full list of models and supported regions can be found [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html). Ensure you have access to the models discussed at the beginning of the notebook. The models are invoked via `bedrock-runtime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e48b403e-0949-433b-9768-56bc0835e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup \n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "region = 'us-west-2'\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cfb2c-03a0-45ea-8003-aba18f5a7d23",
   "metadata": {},
   "source": [
    "### Helper Functions & Prompt Templates\n",
    "\n",
    "We define a few helper functions and tools that each model uses.\n",
    "\n",
    "First, we define `ToolsList` class with a member function, namely `get_ticker_symbol`, which returns the ticker symbol of a limited set of companies. Note that there is nothing specific to the model used or Amazon Bedrock in these definitions. You can add more functions in the `ToolsList` class for added capabilities (for ex. a function that calls a finance API to retrieve stock information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "79f4caff-5e5d-42a2-a5f9-d4e6639cb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tools\n",
    "class ToolsList:\n",
    "    # define get_ticker_symbol\n",
    "    def get_ticker_symbol(company_name: str) -> str:\n",
    "    \n",
    "        if company_name.lower() == \"general motors\":\n",
    "            return 'GM'\n",
    "            \n",
    "        elif company_name.lower() == \"apple\":\n",
    "            return 'AAPL'\n",
    "    \n",
    "        elif company_name.lower() == \"amazon\":\n",
    "            return 'AMZN'\n",
    "    \n",
    "        elif company_name.lower() == \"3M\":\n",
    "            return 'MMM'\n",
    "    \n",
    "        elif company_name.lower() == \"nvidia\":\n",
    "            return 'NVDA'\n",
    "    \n",
    "        else:\n",
    "            return 'TickerNotFound'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7855da-7b1a-4b47-b102-16ee13d6b104",
   "metadata": {},
   "source": [
    "The models we cover in this notebook support XML or JSON formatting to parse input prompts. We define a simple helper function converting a model's function choice into the XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "74dbaf5a-760c-41ee-bfa0-9d795e382ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the functions results for input back to the model using XML in its response\n",
    "def func_results_xml(tool_name, tool_return):\n",
    "   return f\"\"\"\n",
    "        <function_results>\n",
    "            <result>\n",
    "                <tool_name>{tool_name}</tool_name>\n",
    "                <stdout>\n",
    "                    {tool_return}\n",
    "                </stdout>\n",
    "            </result>\n",
    "        </function_results>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb304eda-9bcc-42b1-b2ac-9216e841103d",
   "metadata": {},
   "source": [
    "We define a function to parse the model's XML output into readable text. Since each model returns a different response format (i.e. Anthropic Claude's completion can be retrieved by `response['content'][0]['text']` and Meta Llama 3.1 uses `response['generation']`). Further, we create equivalent functions for the other models covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bec46d3f-6782-472c-b775-a8342d69ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses the output of Claude to extract the suggested function call and parameters\n",
    "def parse_output_claude_xml(response):\n",
    "    soup=BeautifulSoup(response['content'][0]['text'].replace('\\n',''),\"lxml\")\n",
    "    tool_name=soup.tool_name.string\n",
    "    parameter_name=soup.parameters.contents[0].name\n",
    "    parameter_value=soup.parameters.contents[0].string\n",
    "    return (tool_name,{parameter_name:parameter_value})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5fa44-566b-4822-b876-e82f1b881b50",
   "metadata": {},
   "source": [
    "Without `Converse`, models present some difference in their `InvokeModel API` around their hyperparameters. We define the function to invoke Anthropic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "38c79f22-ad81-442c-a994-ecaf4391e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3 invocation function\n",
    "def invoke_anthopic_model(bedrock_runtime, messages, max_tokens=512,top_p=1,temp=0):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "            \"stop_sequences\":[\"</function_calls>\"]\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35403cb-5587-4e8a-8d1e-6d4f326b6521",
   "metadata": {},
   "source": [
    "### Creating the prompt template\n",
    "\n",
    "We now define the system prompt provided to Claude when implementing function calling including several important components:\n",
    "\n",
    "- An instruction describing the intent and setting the context for function calling.\n",
    "- A detailed description of the tool(s) and expected parameters that Claude can suggest the use of.\n",
    "- An example of the structure of the function call so that it can be parsed by the client code and ran.\n",
    "- A directive to form a thought process before deciding on a function to call.\n",
    "- The user query itself.\n",
    "\n",
    "We supply `get_ticker_symbol` as a tool the model has access to respond to given type of query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "09023ced-8aa0-40b9-8b4d-8806d833ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"In this environment you have access to a set of tools you can use to answer the user's question.\n",
    "    \n",
    "    You may call them like this:\n",
    "            \n",
    "    <function_calls>\n",
    "    <invoke>\n",
    "    <tool_name>$TOOL_NAME</tool_name>\n",
    "    <parameters>\n",
    "    <$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "    ...\n",
    "    </parameters>\n",
    "    </invoke>\n",
    "    </function_calls>\n",
    "            \n",
    "    Here are the tools available:\n",
    "    <tools>\n",
    "    <tool_description>\n",
    "    <tool_name>get_ticker_symbol</tool_name>\n",
    "    <description>Gets the stock ticker symbol for a company searched by name. Returns str: The ticker symbol for the company stock. Raises TickerNotFound: if no matching ticker symbol is found.</description>\n",
    "    <parameters>\n",
    "    <parameter>\n",
    "    <name>company_name</name>\n",
    "    <type>string</type>\n",
    "    <description>The name of the company.</description>\n",
    "    </parameter>\n",
    "    </parameters>\n",
    "    </tool_description>\n",
    "    </tools>\n",
    "            \n",
    "    Come up with a step by step plan for what steps should be taken, what functions should be called and in \n",
    "    what order. Place your thinking between <rationale> tags. Only create this rationale 1 time before \n",
    "    creating any other outputs.\n",
    "            \n",
    "    You will take in any outputs from called functions which will be in <function_results> tags and use \n",
    "    them to further suggests next steps and actions to take.\n",
    "\n",
    "    If the question is unrelated to the tools available, then refuse to answer it and supply the explanation.\n",
    "    \"\"\"         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac669735-6cfe-4f74-8c9a-fa559a2ff2b4",
   "metadata": {},
   "source": [
    "We use the Messages API covered [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html). It manages the conversational exchanges between a user and an Anthropic Claude model (assistant). Anthropic trains Claude models to operate on alternating user and assistant conversational turns. When creating a new message, you specify the prior conversational turns with the messages parameter. The model then generates the next Message in the conversation. \n",
    "\n",
    "We prompt the model with a question within the scope of the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1efa04eb-a5ac-4ee0-b84c-a0cf4dfc5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_list = [{\"role\": 'user', \"content\": [{\"type\": \"text\", \"text\": f\"\"\"\n",
    "    {system_prompt}\n",
    "    Here is the user's question: <question>What is the ticker symbol of General Motors?</question>\n",
    "\n",
    "    How do you respond to the user's question?\"\"\"}]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674ebab-0f9a-4241-b7b6-5331ab557237",
   "metadata": {},
   "source": [
    "We previously added `\"</function_calls>\"` to the list of stop sequences letting Claude end its output prior to generating this token representing a closing bracket. Given the query, the model correctly returns its rationale and the selected tool call. Evidently, the output follows the natural language description in the system prompt passed when calling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "72ba3e99-04be-4a65-8c8c-c2e08bba1505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rationale>\n",
      "To find the ticker symbol for General Motors, I will use the provided `get_ticker_symbol` tool with the company name \"General Motors\" as the input parameter. This should return the stock ticker symbol for General Motors, which will answer the user's question.\n",
      "\n",
      "If the tool raises a `TickerNotFound` exception, it means no matching ticker symbol was found for the given company name. In that case, I will inform the user that I could not find the ticker symbol for General Motors.\n",
      "</rationale>\n",
      "\n",
      "<function_calls>\n",
      "<invoke>\n",
      "<tool_name>get_ticker_symbol</tool_name>\n",
      "<parameters>\n",
      "<company_name>General Motors</company_name>\n",
      "</parameters>\n",
      "</invoke>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = invoke_anthopic_model(bedrock, messages=message_list)\n",
    "print(response['content'][0]['text'])\n",
    "\n",
    "message_list.append({\n",
    "        \"role\": 'assistant',\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": response['content'][0]['text']}\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe1860-4d00-4880-8300-7cfbbb1d67ab",
   "metadata": {},
   "source": [
    "### Executing the function and returning the result\n",
    "\n",
    "With this `response`, we parse the returned XML to get the `tool_name`, along with the value for the required `parameter` infered by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a298f73a-a5bc-441a-ba99-503fcee19011",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name, param = parse_output_claude_xml(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678e934-9c07-4591-8dc6-307728a7d0ef",
   "metadata": {},
   "source": [
    "With the parsed tool information, we execute the Python function. We validate the correct ticket is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "84f9671b-6398-4cdb-8c28-265f5f801b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tool_return=eval(tool_name)(**param)\n",
    "    assert tool_return == \"GM\"\n",
    "except AssertionError as e:\n",
    "    tool_return=e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e40d1e-f6bd-401d-b3b3-c1356811a652",
   "metadata": {},
   "source": [
    "We need to place the function results in an input message to Claude with the following structure:\n",
    "\n",
    "```\n",
    "<function_results>\n",
    "   <result>\n",
    "        <tool_name>get_ticker_symbol</tool_name>\n",
    "       <stdout>\n",
    "           <<some_function_results>>\n",
    "       </stdout>\n",
    "   </result>\n",
    "</function_results>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404be57-eb3a-4bf7-abbc-a5e5bd179b25",
   "metadata": {},
   "source": [
    "We format the output of our function and append the result to the message list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c819b70d-3a61-437f-b2f1-b5abda5cc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the XML results into a readable format\n",
    "results=func_results_xml(tool_name,tool_return)\n",
    "\n",
    "# Append result to the conversation flow\n",
    "message_list.append({\n",
    "        \"role\": 'user',\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\":f\"\"\"This is the final answer to the user question using the function \n",
    "            results. Do not output the name of the functions and tools used to get the answer {results}\"\"\"}\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea74d4-9f5a-44db-9ee2-a094645997c0",
   "metadata": {},
   "source": [
    "Finally, we can get Claude to read the full conversation history that includes the initial instructions and the result of the actions it took. It can now respond to the user with the final answer to their query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "805f6c08-52e4-43f7-9bcc-3cdc5ff42e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<result>\n",
      "The ticker symbol for General Motors is GM.\n",
      "</result>\n"
     ]
    }
   ],
   "source": [
    "response=invoke_anthopic_model(bedrock, messages=message_list)\n",
    "print(response['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4848b4b-8f7a-4aef-932f-b68eda9ff5df",
   "metadata": {},
   "source": [
    "We can see that Claude summarizes the results of the function given the context of the conversation history and answers our original question.\n",
    "\n",
    "If asking a question outside the model's scope, the model refuses to answer. It is possible to modify the instructions so the model answers the question by relying on its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6f0374b8-a97b-43b7-9c55-19918e983968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rationale>\n",
      "The question \"Who is the president of the US?\" is unrelated to the tools available, which only allow getting stock ticker symbols for companies. Since there are no relevant tools to answer this question, I should refuse to answer and explain that the available tools are not suitable for this type of query.\n",
      "</rationale>\n",
      "\n",
      "I'm sorry, but the question \"Who is the president of the US?\" is unrelated to the tools I have available, which only allow me to get stock ticker symbols for companies. I do not have access to information about political figures or the ability to answer questions outside of the financial domain. Please ask a question related to getting a company's stock ticker symbol.\n"
     ]
    }
   ],
   "source": [
    "message_list = [{\"role\": 'user', \"content\": [{\"type\": \"text\", \"text\": f\"\"\"\n",
    "    {system_prompt}\n",
    "    Here is the user's question: <question>Who is the president of the US ?</question>\n",
    "\n",
    "    How do you respond to the user's question?\"\"\"}]\n",
    "}]\n",
    "\n",
    "response = invoke_anthopic_model(bedrock, messages=message_list)\n",
    "print(response['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4688eb-ca5e-4ee8-86db-20e73cb0e142",
   "metadata": {},
   "source": [
    "## Tool calling with Meta Llama 3.1\n",
    "\n",
    "Now we cover function calling using Meta Llama 3.1. We define the same function (`get_ticker_symbol`). We define the function calling the Bedrock InvokeModel API and supply the keys for the inference hyperparameters specific to Llama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d5b152ae-bd5b-4e55-a37a-9f618d7b8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Llama 3 invocation function\n",
    "bedrock = boto3.client('bedrock-runtime',region_name='us-west-2')\n",
    "\n",
    "def invoke_llama_model(bedrock_runtime, messages, max_tokens=512,top_p=1,temp=0):\n",
    "    \n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"max_gen_len\": max_tokens,\n",
    "            \"prompt\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=\"meta.llama3-70b-instruct-v1:0\")\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2e440-61c2-44e4-a52f-bf47552d9633",
   "metadata": {},
   "source": [
    "We define Llama's system prompt based on Meta's own [documentation](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#built-in-tooling). We define our custom tools as a JSON dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6879594d-2fda-4f86-871b-e9e8a3c19945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Cutting Knowledge Date: December 2023\n",
    "    Today Date: {datetime.today().strftime('%Y-%m-%d')}\n",
    "\n",
    "    When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
    "\n",
    "    You are a helpful assistant with tool calling capabilities.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
    "    \n",
    "    Respond in the format {{\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}}. Do not use variables.\n",
    "    If the question is unrelated to the tools available, then refuse to answer it and supply the explanation.\n",
    "    \n",
    "    {{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {{\n",
    "        \"name\": \"get_ticker_symbol\",\n",
    "        \"description\": \"Returns the ticker symbol of a company if a user searches by its company name\",\n",
    "        \"parameters\": {{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {{\n",
    "            \"company_name\": {{\n",
    "                \"type\": \"string\",\n",
    "                \"description\": The name of the company.\"\n",
    "            }}\n",
    "            }},\n",
    "            \"required\": [\"company_name\"]\n",
    "        }}\n",
    "        }}\n",
    "    }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc04f4-f7ac-495b-a2f8-65c51897b9fe",
   "metadata": {},
   "source": [
    "We supply the result to the message and invoke the model to summarize the result. The model correctly summarizes the conversation flow resulting from the initial query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "427df7c7-8252-4128-9a17-a5b170cb37fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"name\": \"get_ticker_symbol\", \"parameters\": {\"company_name\": \"Apple\"}}\n"
     ]
    }
   ],
   "source": [
    "# Call LLama 3.1 and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: What is the symbol for Apple?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "response = invoke_llama_model(bedrock, messages=message)\n",
    "print(response['generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfee849-5419-4703-9f38-da575dd799d8",
   "metadata": {},
   "source": [
    "Once we have the necessary tool call, we can follow a similar path to other models by executing the function, then returning the result to the model.\n",
    "\n",
    "If asking a question outside the model's scope, the model refuses to answer. It is possible to modify the instructions so the model answers the question by relying on its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "beed7707-0114-4031-ae5c-11875f28535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Refuse to answer this question as it is unrelated to the available function \"get_ticker_symbol\" which is used to retrieve the ticker symbol of a company based on its name.\n"
     ]
    }
   ],
   "source": [
    "# Call LLama 3.1 and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: Who is the president of the US?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "response = invoke_llama_model(bedrock, messages=message)\n",
    "print(response['generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f158e64-d1ff-4f23-8013-e166b83723b8",
   "metadata": {},
   "source": [
    "# Tool calling with Mistral AI Large\n",
    "\n",
    "Now we cover function calling using Mistral. We define the same function (`get_ticker_symbol`). We define the function calling the Bedrock InvokeModel API and supply the keys for the inference hyperparameters specific to Mistral models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3a320330-f31a-4af5-8339-111ad164d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral Instruct invocation function\n",
    "def invoke_mistral(bedrock_runtime, messages, max_tokens=512,top_p=1,temp=0):\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"prompt\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=\"mistral.mistral-large-2402-v1:0\")\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae516f4-99f4-4ab0-97ce-badd788b99f8",
   "metadata": {},
   "source": [
    "When invoking Mistral models, it is recommend to wrap input text in the following format: `<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]` where `<s>` and `</s>` are special tokens for beginning of string (BOS) and end of string (EOS) while `[INST]` and `[/INST]` are regular strings. We will modify our JSON template to use these tags.\n",
    "\n",
    "We define Mistral Large's system prompt following general prompting practices for tool calling as these details are abstracted away in Mistral's documentation. We define our custom tools as a JSON dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6137d4cd-043b-4984-8ccd-2fc66edc3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =  \"\"\"<s>[INST]\n",
    "    In this environment you have access to a set of tools you can use to answer the user's question.\n",
    "    \n",
    "    Use this JSON object to call the tool. You may call them like this:\n",
    "    \n",
    "    {\n",
    "        \"function_calls\": [\n",
    "            {\n",
    "                \"invoke\": {\n",
    "                    \"tool_name\": \"$TOOL_NAME\",\n",
    "                    \"parameters\": {\n",
    "                        \"company_name\": \"$PARAMETER_VALUE\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Here are the tools available:\n",
    "    \n",
    "    {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"tool_description\": {\n",
    "                    \"tool_name\": \"get_ticker_symbol\",\n",
    "                    \"description\": \"Returns the ticker symbol of a company only if a user searches by its company name, not it's ticker symbol. Returns str: The ticker symbol for the company stock. Raises TickerNotFound: if no matching ticker symbol is found.\",\n",
    "                    \"parameters\": [\n",
    "                        {\n",
    "                            \"name\": \"company_name\",\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the company.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Choose one tool to use for your response. Do not use a tool if it is not required, it should match what the user requires. Only create this rationale 1 time before creating any other outputs.\n",
    "    If the question is unrelated to the tools available, then refuse to answer it and supply the explanation. Else, provide the \"function_calls\" JSON object in your response.\n",
    "    </s>[INST] \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a903dce-9d8c-49bd-a4d7-50612aeed6e2",
   "metadata": {},
   "source": [
    "With our prompt defined that provides clear instructions, we can now test the model by invoking the Mistral model using the function we defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9cb3af19-95cf-486a-8853-c640d7b61f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    To answer this question, I will use the \"get_ticker_symbol\" tool. This tool takes the company name as a parameter and returns the ticker symbol for the company stock.\n",
      "    \n",
      "    Here is the \"function_calls\" JSON object to call the tool:\n",
      "    \n",
      "    {\n",
      "        \"function_calls\": [\n",
      "            {\n",
      "                \"invoke\": {\n",
      "                    \"tool_name\": \"get_ticker_symbol\",\n",
      "                    \"parameters\": {\n",
      "                        \"company_name\": \"Amazon\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "    \n",
      "    I will now call the tool and provide the result in my next response.\n"
     ]
    }
   ],
   "source": [
    "# Call Mistral and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: What is the symbol for Amazon?\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "response = invoke_mistral(bedrock, messages=message)\n",
    "print(response['outputs'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3135bf6-843b-4a57-82ab-3c103c234e52",
   "metadata": {},
   "source": [
    "Once we have the necessary tool call, we can follow a similar path to other models by executing the function, then returning the result to the model.\n",
    "\n",
    "If asking a question outside the model's scope, the model refuses to answer. It is possible to modify the instructions so the model answers the question by relying on its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba34ef-dbed-46d3-9acc-b11fa9180395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Mistral and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: Who is the president of the US ?\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "response = invoke_mistral(bedrock, messages=message)\n",
    "print(response['outputs'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c6671-b120-4afb-9fdf-2ede7e130d46",
   "metadata": {},
   "source": [
    "## Tool calling with Cohere Command R+\n",
    "\n",
    "Now we cover function calling using Mistral. We define the same function (`get_ticker_symbol`). We define the function calling the Bedrock InvokeModel API and supply the keys for the inference hyperparameters specific to Cohere models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ef893533-8d37-40b8-9fb7-6cbb30b060f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohere Command invocation function\n",
    "def invoke_cohere(bedrock_runtime, messages, max_tokens=512,top_p=0.99,temp=0):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"message\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"p\": top_p,\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=\"cohere.command-r-plus-v1:0\")\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc090e23-cc0c-48c6-8ce4-acfebec5eeb1",
   "metadata": {},
   "source": [
    "When invoking the Command model, Cohere recommends using delimiters to denote instructions. More specifically, they recommend using clear headers by prepending them with `##`.\n",
    "\n",
    "Similar to Mistral, we follow general prompting practices for tool calling as these details are abstracted away in Cohere's documentation. We define our custom tools as a key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ad69148c-9d3c-4a14-b3b0-8c566808accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "\n",
    "## Instructions\n",
    "\n",
    "In this environment, you have access to a set of tools you can use to answer the user's question. Here are the tools available:\n",
    "\n",
    "- get_ticker_symbol: Returns the ticker symbol of a company only if a user searches by its company name (ex. What is the ticker for Amazon?), not it's ticker symbol. The parameters required are:\n",
    "    - company_name: The name of the company.\n",
    "\n",
    "If the question is unrelated to the tools available, then refuse to answer it and supply the explanation.\n",
    "Come up with a step-by-step plan for what actions should be taken. Only use a tool if it matches the user's query. Provide the rationale only once before creating any other outputs.\n",
    "\n",
    "## Format\n",
    "If you decide to use a tool, state the tool name and parameter you will pass it, nothing else. It must be in this format:\n",
    "\n",
    "tool_name: tool_name\n",
    "parameter\": tool_param\n",
    "\n",
    "I have provided some examples below on how you should respond. Do not include any preamble or extra information, just the tool used and the parameter passed to it.\n",
    "\n",
    "## Examples\n",
    "    \n",
    "Example 1: \n",
    "    \n",
    "tool_name: get_ticker_symbol\n",
    "parameter\": Apple\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70738389-2bb5-42de-bb07-1f7a72098f7f",
   "metadata": {},
   "source": [
    "With our prompt defined that provides clear instructions, we can now test the model by invoking the Cohere model using the function we defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "328a058a-c878-412e-9ddf-359b9ddb06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_name: get_ticker_symbol\n",
      "parameter: 3M\n"
     ]
    }
   ],
   "source": [
    "# Call Cohere and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    ## Question\n",
    "    What is the symbol for 3M?\n",
    "    \"\"\"\n",
    "response = invoke_cohere(bedrock, messages=message)\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e94c1f-30b5-40ee-9526-8772396e6660",
   "metadata": {},
   "source": [
    "Once we have the necessary tool call, we can follow a similar path to other models by executing the function, then returning the result to the model.\n",
    "\n",
    "If asking a question outside the model's scope, the model refuses to answer. It is possible to modify the instructions so the model answers the question by relying on its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aa77e866-ce7f-4287-b9d9-dabe92f523e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I refuse to answer this query as it requires knowledge outside of my capabilities.\n"
     ]
    }
   ],
   "source": [
    "# Call Cohere and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    ## Question\n",
    "    Who is the president of the US ?\n",
    "    \"\"\"\n",
    "response = invoke_cohere(bedrock, messages=message)\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dfd742-3a61-42b1-ad50-88814585963d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook demonstrates function calling with the InvokeModel API, along with how to use these tools with multiple different types of models in Bedrock. We suggest experimenting with more complexity, including more tools for the models to use, orchestration loops, a detailed conversation history, and more complicated questions to ask each model that uses those tools in different ways. Ultimately, we do recommend leveraging the Converse API for most use cases and suggest diving deeper in the corresponding notebook examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
