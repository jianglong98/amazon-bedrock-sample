{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b18e05-3845-40be-8d56-0c510f2c05c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eff0d0-1b42-4237-9e3c-35ca6a315367",
   "metadata": {},
   "source": [
    "# Start the provisioned throughput for the finetuned model and make inferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fb2ee-f9ad-4135-b3f3-550b97d41629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d066b09-1ae5-4211-ac4c-4c3a0ca70650",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c8b8d-e3a1-4160-94ca-5f45c8ccaaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import logging\n",
    "from enum import Enum\n",
    "import ast\n",
    "import boto3\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9412a-b07c-4270-bfae-dc8e33f81e2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define our tools\n",
    "\n",
    "We will need these to test out our endpoint!\n",
    "\n",
    "To properly train our model on tool usage we need to define our tool definitions. We can do so by defining functions with explicit typed inputs and structured docstrings. \n",
    "\n",
    "We are going to define 8 tools:\n",
    "- weather_api_call\n",
    "- stat_pull\n",
    "- text_to_sql\n",
    "- terminal\n",
    "- wikipedia\n",
    "- duckduckgo_results_json\n",
    "- youtube_search\n",
    "- pubmed_search\n",
    "\n",
    "While we are defining 8 tools, we are only going to train our model on 7 of them. This is so that we can test out our performance on unseen tools after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadd51a-0eab-45ff-ac71-d95cc195a424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import weather_api_call, stat_pull,terminal,text_to_sql,wikipidea,youtube_search, pubmed_search, duckduckgo_results_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dd2ad-d3c7-4dfd-9936-a0e30122188e",
   "metadata": {},
   "source": [
    "### Deploy fine tuned model using bedrock \n",
    "\n",
    "Retrieve the fine-tuned model ID from the fine tuning jobâ€™s output, and create a Provisioned Throughput model instance with the desired model units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d28a2-3eca-4b09-8018-7cdb2e3bff2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROVISIONED_MODEL_NAME =\"\" # change accordingly\n",
    "FT_MODEL_ARN= \"\" # check the FT model ARN from Amazon Bedrock after FT finishes\n",
    "\n",
    "my_config = Config(\n",
    "    region_name = 'us-east-1',\n",
    "    retries = {\n",
    "        'max_attempts': 5,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\", config=my_config)\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "\n",
    "# create provisioned model for the ft model (remember to stop it later)\n",
    "provisioned_model_id = bedrock.create_provisioned_model_throughput(\n",
    "                                    modelUnits=1,\n",
    "                                    provisionedModelName=PROVISIONED_MODEL_NAME,\n",
    "                                    modelId= FT_MODEL_ARN\"\n",
    "                        )\n",
    "print(provisioned_model_id['provisionedModelArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cb95f-5677-4d0c-ab60-3b7ecdffebf6",
   "metadata": {},
   "source": [
    "### FT Amazon Nova model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419912f-d843-4cf8-b9f4-b7cc2ca8a44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = provisioned_model_id['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7ff8b-9f6b-4474-8281-48df491dff74",
   "metadata": {},
   "source": [
    "### Define the system prompt and messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036a395-0fbc-4861-9c80-98343a003f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sys_msg =\"\"\"You are a bot that can handle different requests with tools.\"\"\"\n",
    "system_prompt = [{\"text\": sys_msg}]\n",
    "\n",
    "# Prepare the tool configuration with the weather tool's specification\n",
    "tool_config = {\"tools\": [weather_api_call.get_tool_spec(),\n",
    "                         stat_pull.get_tool_spec(),\n",
    "                         terminal.get_tool_spec(),\n",
    "                         text_to_sql.get_tool_spec(),\n",
    "                         wikipidea.get_tool_spec(),\n",
    "                         youtube_search.get_tool_spec(),\n",
    "                         pubmed_search.get_tool_spec(),\n",
    "                         duckduckgo_results_json.get_tool_spec()                        \n",
    "                        ]\n",
    "              }\n",
    "\n",
    "# appropriate prompt template for tool calling \n",
    "\n",
    "promt_template = \"\"\"\n",
    "Given the following functions within <tools>, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
    "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables. Donot give any explanations. \n",
    "ONLY output the resulting JSON structure and nothing else.Donot use the word 'json' anywhere in the result.\n",
    "\n",
    "<tools>{tool_config}</tools>\n",
    "\n",
    "Generate answer for the following question.\n",
    "<question>{question}</question>\n",
    "\"\"\"\n",
    "# Convert tools configuration to JSON string\n",
    "formatted_tool_config = json.dumps(tool_config, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac7d3f-5017-4ff0-b5e7-f052892efc54",
   "metadata": {},
   "source": [
    "### Test single question\n",
    "\n",
    "Let's run a single question through our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51156261-7b60-46df-bccb-39d1dfa8d25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a single question to check inference\n",
    "question = \"What research is available on the effects of music therapy for autism spectrum disorders?\"\n",
    "\n",
    "prompt = promt_template.replace(\"{question}\", question)\n",
    "prompt = prompt.replace(\"{tool_config}\", formatted_tool_config)\n",
    "\n",
    "# define model_kwargs\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                   \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "max_tokens= 4096\n",
    "temperature= 0.2\n",
    "inferenceConfig = {\n",
    "                \"max_new_tokens\": max_tokens,\n",
    "                \"temperature\": temperature, \n",
    "                # \"top_p\": float,\n",
    "                # \"top_k\": 1\n",
    "            }\n",
    "\n",
    "# Prepare request body\n",
    "model_kwargs = {\"system\":system_prompt,\n",
    "                \"messages\": messages,\n",
    "                 \"inferenceConfig\": inferenceConfig,}\n",
    "body = json.dumps(model_kwargs)\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "# invoke the model to make inference\n",
    "response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "ft_response_body = json.loads(response.get(\"body\").read())\n",
    "# Parse response\n",
    "ft_response_text = ft_response_body['output']['message']['content'][0]['text']\n",
    "ast.literal_eval(ft_response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff9b5d-bcb0-4b98-a266-c93b1e9b5547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0053d90-fa37-4f7c-86ee-f1002ab889c6",
   "metadata": {},
   "source": [
    "### Load our test set\n",
    "\n",
    "Let's load our full test set that we can run inference against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3377b-84d6-4abc-8bc1-10ec8dc26321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question_bank_path = \"../assets/bedrock_nova_ft/test_ft.jsonl\"\n",
    "\n",
    "\n",
    "test_question_list_jsonl = []\n",
    "with open(test_question_bank_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object\n",
    "        line = json.loads(line.strip())\n",
    "        test_question_list_jsonl.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cab77c-1ba7-4af6-94d3-37d8d3014963",
   "metadata": {},
   "source": [
    "### Run test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8877c-6475-4395-bcad-4ed373243142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "output_list = []\n",
    "elapsed_time = 0 \n",
    "data =[]\n",
    "for question_dict in tqdm(test_question_list_jsonl):# Next, create a chat and apply the chat template\n",
    "    \n",
    "    temp={}\n",
    "    question = question_dict['messages'][0]['content']\n",
    "    prompt = promt_template.replace(\"{question}\", question)\n",
    "    prompt = prompt.replace(\"{tool_config}\", formatted_tool_config)\n",
    "    # define model_kwargs\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                   \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    max_tokens= 4096\n",
    "    temperature= 0.2\n",
    "    inferenceConfig = {\n",
    "                \"max_new_tokens\": max_tokens,\n",
    "                \"temperature\": temperature, \n",
    "                # \"top_p\": float,\n",
    "                # \"top_k\": 1\n",
    "            }\n",
    "\n",
    "    # Prepare request body\n",
    "    model_kwargs = {\"system\":system_prompt,\n",
    "                \"messages\": messages,\n",
    "                 \"inferenceConfig\": inferenceConfig,}\n",
    "    body = json.dumps(model_kwargs)\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"    \n",
    "    # invoke the model to make inference\n",
    "    start_time = time.time()     \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "        )   \n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    ft_response_body = json.loads(response.get(\"body\").read())\n",
    "    \n",
    "    in_tok = ft_response_body[ 'usage' ]['inputTokens']\n",
    "    out_tok = ft_response_body[ 'usage' ]['outputTokens']   #response['ResponseMetadata']['HTTPHeaders']['x-amzn-bedrock-output-token-count']\n",
    "    tot_tok = ft_response_body[ 'usage' ]['totalTokens']\n",
    "    ft_response_text = ft_response_body['output']['message']['content'][0]['text']\n",
    "        \n",
    "    print(f\"{in_tok}, {out_tok}, {tot_tok},{elapsed_time}\\n\")\n",
    "    # Parse response\n",
    "    ft_response_text = ft_response_body['output']['message']['content'][0]['text']\n",
    "    temp['user_question'] =question       \n",
    "    temp['response']= ft_response_text\n",
    "    temp['input_tokens']= in_tok\n",
    "    temp['output_tokens']= out_tok\n",
    "    temp['total_tokens']= tot_tok\n",
    "    temp['latency']= elapsed_time\n",
    "    data.append(temp)\n",
    "    output_list.append(ft_response_text)\n",
    "    print(ft_response_text)\n",
    "    time.sleep(5)\n",
    "   \n",
    "\n",
    "print(f\" avg latency {round((elapsed_time/len(test_question_list_jsonl)),2)} \\n\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d855e2-c9f3-4f4f-8797-254bada475f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# Get the keys for the CSV header (assuming all dictionaries have the same keys)\n",
    "fieldnames = data[0].keys()\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "output_file='./results/micro_output.csv'\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    # Create a DictWriter object, specifying the fieldnames\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)   \n",
    "    writer.writeheader()    \n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been written to {output_file} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1e7b4-ad85-4c4d-92f0-d1c231ce65b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parsing the output\n",
    "print(eval(output_list[0]))\n",
    "print(eval(output_list[0])['name'])\n",
    "print(eval(output_list[0])['parameters'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbfc4e-3f2d-4a88-b06b-5e4855a3d67f",
   "metadata": {},
   "source": [
    "### Assess tool calling accuracy\n",
    "\n",
    "Let's now grade our model on its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949acc27-3e5e-41e8-b6ee-97e50413c272",
   "metadata": {},
   "source": [
    "## new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfda2e6-a497-44ef-8d63-11ea84c43b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question_bank_path = \"../assets/test_data.txt\"\n",
    "\n",
    "test_question_list = []\n",
    "with open(test_question_bank_path) as f:\n",
    "    for line in f.readlines():\n",
    "        test_question_list.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99755957-ecd9-4079-9eed-9e712c8be6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b28fa-c9ad-4fe2-bedd-d75f87b80af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "verbose = True\n",
    "# need to have a way to evaluate both regex patterns and inclusives \n",
    "# maybe you have the regex have an index it corresponds to?\n",
    "\n",
    "remap_dict = {\n",
    "    \"terminal\":\"shell_tool\",\n",
    "    \"wikipedia\":\"wiki_tool\",\n",
    "    \"youtube_search\":\"youtube_tool\",\n",
    "    \"pubmed_search\":\"pubmed_tool\",\n",
    "    \"stat_pull\":\"stat_pull\",\n",
    "    \"text_to_sql\":\"text_to_sql\",\n",
    "    \"create_plan\":\"create_plan\",\n",
    "    \"duckduckgo_results_json\":\"internet_search_tool\",\n",
    "    \"weather_api_call\":\"weather_api_call\",\n",
    "}\n",
    "\n",
    "# need to have set numbers of correct arguments/tool calls\n",
    "correct_tool = [0] * len(test_question_list)\n",
    "correct_args = [0] * len(test_question_list)\n",
    "for i, question_dict in tqdm(enumerate(test_question_list)):\n",
    "    # loop through the questions\n",
    "    \n",
    "    question =  question_dict['question']\n",
    "    answer =  question_dict['answer']\n",
    "    args = question_dict[\"args\"]\n",
    "    tool_keys = list(args.keys())\n",
    "    num_tools = len(tool_keys)\n",
    "    print(f\"User question: {question}\\n\")\n",
    "    \n",
    "    try:\n",
    "        out_dict = eval(output_list[i]) #llm output list\n",
    "       \n",
    "        try:\n",
    "            # give credit to printing out a function instead of function string\n",
    "            name = str(out_dict['name'].__name__)\n",
    "            \n",
    "            tool_calls = out_dict['parameters']\n",
    "        except:\n",
    "            name = str(out_dict['name'])\n",
    "            \n",
    "            tool_calls = out_dict['parameters']\n",
    "\n",
    "        \n",
    "        print(f\"GT tool: {answer}   LLM output tool: {name} \\n\")\n",
    "        \n",
    "        if name == answer:\n",
    "            correct_tool[i] = 1\n",
    "        else:\n",
    "            print(\"TOOL FAIL\")\n",
    "            correct_tool[i] = 0\n",
    "        \n",
    "        for j, tool_key in enumerate(tool_keys): # need to loop through the tool arguments \n",
    "            if (isinstance(tool_calls[tool_key], list))&(len(args[tool_key])>1):\n",
    "                # if multiple arguments, join them together, this is for things like code where a list of args is passed back\n",
    "                pred_args = [\" && \".join(tool_calls[tool_key])]\n",
    "                if verbose:\n",
    "                    print(\"new args: \", pred_args)\n",
    "            elif isinstance(tool_calls[tool_key], str):\n",
    "                # add list around tool calls if string\n",
    "                pred_args = [tool_calls[tool_key]]\n",
    "            else:\n",
    "                pred_args = tool_calls[tool_key]\n",
    "            if verbose:\n",
    "                print(\"pred args: \", pred_args)\n",
    "                #print()\n",
    "            if test_question_list[i][\"arg_pattern\"]:\n",
    "                # if there are regex patterns, evaluate them\n",
    "                gt_arg = test_question_list[i][\"arg_pattern\"] \n",
    "                for gt in gt_arg:\n",
    "                    # loop through each valid arg pattern\n",
    "                    if verbose:\n",
    "                        \n",
    "                        print(\"ground truth pattern:\",gt)\n",
    "                        #print()\n",
    "                    if (re.match(gt, pred_args[0].lower()) != None)|(gt == pred_args[0]):\n",
    "                        correct_args[i] += 1/num_tools\n",
    "                        if verbose:\n",
    "                            print(\"regex match\")\n",
    "                            print(\"arg score\", correct_args[i])\n",
    "                            #print()\n",
    "                        break\n",
    "                    else:\n",
    "                        correct_args[i] += 0\n",
    "                        if verbose:\n",
    "                            print(\"Failed regex match\")\n",
    "            else:\n",
    "                # need to loop through \n",
    "                gt_arg = test_question_list[i][\"args\"][tool_key][0].lower()\n",
    "                if gt_arg in pred_args[0].lower():\n",
    "                    correct_args[i] += 1/num_tools\n",
    "                    if verbose:\n",
    "                        print(\"straight match\")\n",
    "                        print(\"arg score\", correct_args[i])\n",
    "\n",
    "                else:\n",
    "                    correct_args[i] += 0\n",
    "                    if verbose:\n",
    "                        print(\"Failed straight match\")\n",
    "            \n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(\"exception\")\n",
    "        correct_tool[i] = 0\n",
    "        correct_args[i] = 0\n",
    "        print(e)\n",
    "    print(\"--------\")\n",
    "          \n",
    "print(np.mean(correct_tool))\n",
    "print(np.mean(correct_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c35feb",
   "metadata": {},
   "source": [
    "### Cleanup and delete the provisioned throughput model you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d4ddf-8cba-4b81-ab1f-b1f4ad4982ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bedrock.delete_provisioned_model_throughput(provisionedModelId=model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
