{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e12e578",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Model Distillation Guide\n",
    "\n",
    "## Introduction\n",
    "Model distillation in Amazon Bedrock allows you to create smaller, more efficient models while maintaining performance by learning from larger, more capable models. This guide demonstrates how to use the Amazon Bedrock APIs to implement model distillation using:\n",
    "**historical model invocation logs**.\n",
    "\n",
    "Through this API usage notebook, we'll explore the complete distillation workflow, from configuring teacher and student models to deploying the final distilled model. You'll learn how to set up distillation jobs, manage training data sources, handle model deployments, and implement production best practices using boto3 and the Bedrock SDK.\n",
    "\n",
    "The guide covers essential API operations including:\n",
    "- Creating and configuring distillation jobs\n",
    "- Invoke model to generate invocation logs using ConverseAPI\n",
    "- Working with historical invocation logs in your account to create distillation job\n",
    "- Managing model provisioning and deployment\n",
    "- Running inference with distilled models\n",
    "\n",
    "\n",
    "While model distillation offers benefits like improved efficiency and reduced costs, this guide focuses on the practical implementation details and API usage patterns needed to successfully execute distillation workflows in Amazon Bedrock.\n",
    "\n",
    "## Best Practices and Considerations\n",
    "\n",
    "When using model distillation:\n",
    "1. Ensure your training data is diverse and representative of your use case\n",
    "2. Monitor distillation metrics in the S3 output location\n",
    "3. Evaluate the distilled model's performance against your requirements\n",
    "4. Consider cost-performance tradeoffs when selecting model units for deployment\n",
    "\n",
    "The distilled model should provide faster responses and lower costs while maintaining acceptable performance for your specific use case.\n",
    "\n",
    "\n",
    "## Setup and Prerequisites\n",
    "\n",
    "Before starting with model distillation, ensure you have the following:\n",
    "\n",
    "#### Required AWS Resources:\n",
    "- An AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing invocation logs \n",
    "- An S3 bucket to store output metrics\n",
    "- Sufficient service quota to use Provisioned Throughput in Bedrock\n",
    "- An IAM role with the following permissions:\n",
    "\n",
    "IAM Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET/*\",\n",
    "                \"arn:aws:s3:::YOUR_INVOCATION_LOG_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_INVOCATION_LOG_BUCKET/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:ListModelCustomizationJobs\",\n",
    "                \"bedrock:StopModelCustomizationJob\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Trust Relationship:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"YOUR_ACCOUNT_ID\"\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Dataset:\n",
    "As an example, in this notebook we will be using Uber10K dataset, which already contains a system prompt and the relevant contexts to the question in each prompt. \n",
    "\n",
    "\n",
    "First, let's set up our environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295906ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "sts_client = session.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf4606",
   "metadata": {},
   "source": [
    "####  Model selection\n",
    "When selecting models for distillation, consider:\n",
    "1. Performance targets\n",
    "2. Latency requirements\n",
    "3. Total Cost of Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0555707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup teacher and student model pairs\n",
    "teacher_model_id = \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "student_model = \"meta.llama3-1-8b-instruct-v1:0:128k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ef8cf",
   "metadata": {},
   "source": [
    "### Step 1. Configure Model Invocation Logging using the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a869312",
   "metadata": {},
   "source": [
    "In this example, we only store loggings to S3 bucket, but you can optionally enable logging in Cloudwatch as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce95174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket and prefix to store invocation logs\n",
    "s3_bucket_for_log = \"<YOUR S3 BUCKET TO STORE INVOCATION LOGS>\"\n",
    "prefix_for_log = \"<PREFIX FOR LOG STORAGE>\" # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_s3_bucket_policy(bucket_name, prefix, account_id, region):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    bucket_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"AmazonBedrockLogsWrite\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"s3:PutObject\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                     f\"arn:aws:s3:::{bucket_name}/{prefix}/AWSLogs/{account_id}/BedrockModelInvocationLogs/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:SourceAccount\": account_id\n",
    "                    },\n",
    "                    \"ArnLike\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:*\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    bucket_policy_string = json.dumps(bucket_policy)\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.put_bucket_policy(\n",
    "            Bucket=bucket_name,\n",
    "            Policy=bucket_policy_string\n",
    "        )\n",
    "        print(\"Successfully set bucket policy\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting bucket policy: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bucket policy\n",
    "setup_s3_bucket_policy(s3_bucket_for_log, prefix_for_log, account_id, region)\n",
    "\n",
    "# Setup logging configuration\n",
    "bedrock_client.put_model_invocation_logging_configuration(\n",
    "    loggingConfig={\n",
    "        's3Config': {\n",
    "            'bucketName': s3_bucket_for_log,\n",
    "            'keyPrefix': prefix_for_log\n",
    "        },\n",
    "        'textDataDeliveryEnabled': True,\n",
    "        'imageDataDeliveryEnabled': True,\n",
    "        'embeddingDataDeliveryEnabled': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861563a",
   "metadata": {},
   "source": [
    "### Step 2. Invoke teacher model to generate logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9a866",
   "metadata": {},
   "source": [
    "We're using [ConverseAPI](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) in this example, but you can also use [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) in Bedrock.\n",
    "\n",
    "We will invoke `Llama3.1 70b` to generate response on `Uber10K` dataset for each input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inference params\n",
    "inference_config = {\"maxTokens\": 2048, \"temperature\": 0.1, \"topP\": 0.9}\n",
    "request_metadata = {\"job_type\": \"Uber10K\",\n",
    "                    \"use_case\": \"RAG\",\n",
    "                    \"invoke_model\": \"llama31-70b\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58144f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The following code sample takes about 30mins to complete, which invokes teacher model to generate invocation logs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SampleData/uber10K.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        prompt = data['prompt']\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=teacher_model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig=inference_config,\n",
    "            requestMetadata=request_metadata\n",
    "        )\n",
    "\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd720171",
   "metadata": {},
   "source": [
    "### Step 3. Configure and submit distillation job using historical invocation logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b1c7e",
   "metadata": {},
   "source": [
    "Now we have enough logs in our S3 bucket, let's configure and submit our distillation job using historical invocation logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe5a41",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Please make sure to update <b>role_arn</b> and <b>output_path</b> in the following code sample\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3bbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique names for the job and model\n",
    "job_name = f\"distillation-job-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "model_name = f\"distilled-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Set maximum response length\n",
    "max_response_length = 1000\n",
    "\n",
    "# Setup IAM role\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\" # Replace by your IAM role configured for distillation job (Update everything starting with < and ending with >)\n",
    "\n",
    "# Invocation_logs_data\n",
    "invocation_logs_data = f\"s3://{s3_bucket_for_log}/{prefix_for_log}/AWSLogs\"\n",
    "output_path = \"s3://<YOUR_BUCKET>/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training data using invocation logs\n",
    "training_data_config = {\n",
    "    'invocationLogsConfig': {\n",
    "        'usePromptResponse': True, # By default it is set as \"False\"\n",
    "        'invocationLogSource': {\n",
    "            's3Uri': invocation_logs_data\n",
    "        },\n",
    "        'requestMetadataFilters': { # Replace by our filter\n",
    "            'equals': {\"job_type\": \"Uber10K\"},\n",
    "            'equals': {\"use_case\": \"RAG\"},\n",
    "            'equals': {\"invoke_model\": \"llama31-70b\"},\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distillation job with invocation logs\n",
    "response = bedrock_client.create_model_customization_job(\n",
    "    jobName=job_name,\n",
    "    customModelName=model_name,\n",
    "    roleArn=role_arn,\n",
    "    baseModelIdentifier=student_model,\n",
    "    customizationType=\"DISTILLATION\",\n",
    "    trainingDataConfig=training_data_config,\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    customizationConfig={\n",
    "        \"distillationConfig\": {\n",
    "            \"teacherModelConfig\": {\n",
    "                \"teacherModelIdentifier\": teacher_model_id,\n",
    "                \"maxResponseLengthForInference\": max_response_length\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe5a47",
   "metadata": {},
   "source": [
    "### Step 4. Monitoring distillation job status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45ac29",
   "metadata": {},
   "source": [
    "After submitted your distillation job, you can run the following code to monitor the job status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59b688",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Please be aware that distillation job could run for up to 7 days\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78caec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the distillation job arn\n",
    "job_arn = response['jobArn']\n",
    "\n",
    "# print job status\n",
    "job_status = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)[\"status\"]\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82104f86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Proceed to following sections only when the status shows <b>Complete</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc9af3",
   "metadata": {},
   "source": [
    "### Step 5. Deploying the Distilled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f326a",
   "metadata": {},
   "source": [
    "After distillation is complete, you'll need to set up Provisioned Throughput to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the distilled model\n",
    "custom_model_id = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)['outputModelArn']\n",
    "distilled_model_name = f\"distilled-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "provisioned_model_id = bedrock_client.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=distilled_model_name,\n",
    "    modelId=custom_model_id \n",
    ")['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the provisioned throughput status, proceed until it shows **InService**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print pt status\n",
    "pt_status = bedrock_client.get_provisioned_model_throughput(provisionedModelId=provisioned_model_id)['status']\n",
    "print(pt_status)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "67503550",
   "metadata": {},
   "source": [
    "### Step 6. Run inference with provisioned throughput units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30ef9a",
   "metadata": {},
   "source": [
    "In this example, we use ConverseAPI to invoke the distilled model, you can use both InvokeModel or ConverseAPI to generate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95150c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference with the distilled model\n",
    "input_prompt = \"<Your input prompt here>\"  # Replace by your input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a28e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [ \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [{\"text\": input_prompt}], \n",
    "    } \n",
    "]\n",
    "inferenceConfig = {\n",
    "    \"maxTokens\": 2048, \n",
    "    \"temperature\": 0.1, \n",
    "    \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "# test the deloyed model\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=provisioned_model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig=inferenceConfig,\n",
    ")\n",
    "response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbdbb1",
   "metadata": {},
   "source": [
    "### (Optional) Model Copy and Share\n",
    "\n",
    "If you want to deploy the model to a `different AWS Region` or a `different AWS account`, you can use `Model Share` and `Model Copy` feature of Amazon Bedrock. Please check the following notebook for more information.\n",
    "\n",
    "[Sample notebook](https://github.com/aws-samples/amazon-bedrock-samples/blob/main_archieve_10_06_2024/custom_models/model_copy/cross-region-copy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 7. Cleanup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After you're done with the experiment, please ensure to **delete** the provisioned throughput model unit to avoid unnecessary cost."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "In this guide, we've walked through the entire process of model distillation using Amazon Bedrock with historical model invocation logs. We covered:\n",
    "\n",
    "1. Setting up the environment and configuring necessary AWS resources\n",
    "2. Configuring model invocation logging using the API\n",
    "3. Invoking the teacher model to generate logs\n",
    "4. Configuring and submitting a distillation job using historical invocation logs\n",
    "5. Monitoring the distillation job's progress\n",
    "6. Deploying the distilled model using Provisioned Throughput\n",
    "7. Running inference with the distilled model\n",
    "8. Optional model copy and share procedures\n",
    "9. Cleaning up resources\n",
    "\n",
    "Remember to always consider your specific use case requirements when selecting models, configuring the distillation process, and filtering invocation logs. The ability to use actual production data from your model invocations can lead to distilled models that are highly optimized for your particular applications.\n",
    "\n",
    "With these tools and techniques at your disposal, you're well-equipped to leverage the power of model distillation to optimize your AI/ML workflows in Amazon Bedrock.\n",
    "\n",
    "**Happy distilling!**"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
